\documentclass[11pt,twoside]{article}
%\documentclass[10pt,twoside,twocolumn]{article}
\usepackage[english]{babel}
\usepackage{times,subeqnarray}
\usepackage{url}
% following is for pdflatex vs. old(dvi) latex
\newif\myifpdf
\ifx\pdfoutput\undefined
%  \pdffalse           % we are not running PDFLaTeX
   \usepackage[dvips]{graphicx}
\else
   \pdfoutput=1        % we are running PDFLaTeX
%  \pdftrue
   \usepackage[pdftex]{graphicx}
\fi
\usepackage{apatitlepages}
% if you want to be more fully apa-style for submission, then use this
%\usepackage{setspace,psypub,ulem}
%\usepackage{setspace} % must come before psypub
%\usepackage{psypub}
\usepackage{psydraft}
%\usepackage{one-in-margins}  % use instead of psydraft for one-in-margs
\usepackage{apa}       % apa must come last
% using latex2e as standard, use the following for latex209
% \documentstyle [times,11pt,twoside,subeqnarray,psydraft,apa,epsf]{article}
\input netsym

% tell pdflatex to prefer .pdf files over .png files!!
\myifpdf
  \DeclareGraphicsExtensions{.pdf,.eps,.png,.jpg,.mps,.tif}
\fi

% use 0 for psypub format 
\parskip 2pt
% for double-spacing, determines spacing 
%\doublespacing
%\setstretch{1.7}

\columnsep .25in   % 3/8 in column separation

\def\myheading{ Learning Through Time }

% no twoside for pure apa style, use \markright with heading only
\pagestyle{myheadings}
\markboth{\hspace{.5in} \myheading \hfill}{\hfill O'Reilly \hspace{.5in}}

\begin{document}
\bibliographystyle{apa}

% sloppy is the way to go!
\sloppy
\raggedbottom

\def\mytitle{ Learning Through Time in the Thalamocortical Loops }

\def\myauthor{Randall C. O'Reilly, Dean Wyatte, John Rohrlich, and Seth A. Herd \\
  Department of Psychology and Neuroscience \\
  University of Colorado Boulder \\
  345 UCB\\
  Boulder, CO 80309\\
  {\small randy.oreilly@colorado.edu}\\}

\def\mynote{Draft Manuscript: Do not cite or quote without
  permission.\\

  Supported by: ONR grant N00014-13-1-0067 (current), ONR N00014-10-1-0177 (prev), ONR D00014-12-C-0638 (hawkins eCortex)

Supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of the Interior (DOI) contract number D10PC20021. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. The views and conclusions contained hereon are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DOI, or the U.S. Government.

  This work utilized the Janus supercomputer, which is supported by the National Science Foundation (award number CNS-0821794) and the University of Colorado Boulder. The Janus supercomputer is a joint effort of the University of Colorado Boulder, the University of Colorado Denver and the National Center for Atmospheric Research.
}

\def\myabstract{
  Now that's abstract..
}

% \titlesepage{\mytitle}{\myauthor}{\mynote}{\myabstract}
% \twocolumn

%\titlesamepage{\mytitle}{\myauthor}{\mynote}{\myabstract}

\titlesamepageoc{\mytitle}{\myauthor}{\mynote}{\myabstract}

% single-spaced table of contents, delete if unwanted
% \newpage
% \begingroup
% \parskip 0em
% \tableofcontents
% \endgroup
% \newpage

% \twocolumn

\pagestyle{myheadings}

%John's comments in no particular order
%
% * Suggest an explicit explanation of continuous vision, 24 frames per second etc and how this interacts with 100 ms learning cycle (in other words a clear explanation of how perception and learning are occurring simultaneously at different paces within the same neurons)
%		* this section addresses the issue tangentially - … gamma oscillations are nested within individual alpha cycles in a local patch of tissue (Spaak, 				Bonnefond, Maier, Leopold, & Jensen, 2012), suggesting that alpha activity (from deep cortical layers) provides the broader temporal context within which 			superficial layer processing…
%		also this section touches on the issue  ...if sensory-threshold information is presented during the wrong part of the alpha cycle it can be rendered 				imperceptible … 
%
% * The paper takes a very narrow view of context and prediction (just prior input). The incorporation of top down context (excitation from higher areas) that is not explicitly a prior input is not mentioned. Not a problem per se but I think we should be clear about the scope of the context and the scope of learning that can occur.
%
% * Also related to scope is lack of mention of cortical scope.
%		* Does the same computation repeat across every cortical area -- sensory and high-level? Prefrontal areas show alpha rhythms too, but it is unclear whether 		they are the same as posterior alpha. 
%		* Is primary sensory cortex special (V1, A1) because it receives directly from the thalamus? How different are the computations for secondary cortices?
%
% * figure 1 - this appears to be a case of accurate prediction - would be helpful to contrast with error case
%
% * figure 3 - hard to grasp - are the colors significant - it looks like possibly yes

How does the neocortex support the remarkable learning abilities that enable humans (and other mammals) to acquire the vast majority of our intelligence?  In this paper, we advance a comprehensive new framework for neocortical learning that leverages the biological properties of the thalamocortical loops (which are ubiquitous throughout the extent of the neocortex) to support {\em predictive learning}: learning from the differences between {\em expectations} versus actual {\em outcomes}.  This framework builds upon our existing neocortical learning framework, {\em Leabra} \cite{OReilly96,OReillyMunakata00,OReillyMunakataFrankEtAl12}, and adds a temporal integration mechanism, so we refer to this new framework as {\em LeabraTI}.  This temporal integration property, which depends on several features of the thalamocortical loops, enables the network to maintain prior temporal context in a way that can be leveraged to form increasingly accurate predictions or expectations about what will happen next.  This framework requires processing to be discretized over time, to distinguish between expectations vs. outcomes as two sequential states of activation, and we find that this fits surprisingly well with the increasing evidence of a strong alpha frequency (10 Hz) modulation of neocortical processing \cite{LorinczKekesiJuhaszEtAl09,FranceschettiGuatteoPanzicaEtAl95,BuffaloFriesLandmanEtAl11,LuczakBarthoHarris13}, and corresponding behavioral data suggesting that perception is also discretized at the alpha frequency \cite{VanRullenKoch03b}.  Overall, we find that this learning framework integrates a wide range of previously unconnected biological and behavioral data, under a coherent, computationally powerful model.  In addition to reviewing and synthesizing this diverse body of empirical data, we demonstrate the computational power of this model in the context of object recognition with cluttered visual displays.

% Dean: Below is way too much information for the intro -- no need to go back 65 years to Hebb, need to get to the point faster!
% ROR: disagree -- we've introduced key point above, and this we refer back to these ideas as things develop, so I think it is important, and not THAT long...

The quest to understand the essential form of human cognitive learning has a long history, including the classical contributions of \incite{James90} and \incite{Hebb49}, which emphasized what we now call the {\em Hebbian} correlational learning principle: ``neurons that fire together, wire together.''  At a broader level, Hebbian learning is thought to drive the encoding of important statistical structure in the external world (e.g., in an {\em internal model} of the environment), which then enables more sophisticated forms of inference or reasoning about the world.  Much of the recent focus on basic mechanisms of neural plasticity at the synaptic level has remained focused on the Hebbian paradigm, including considerable excitement about spike-timing dependent plasticity (STDP) \cite{BiPoo98,MarkramLubkeSakmann97}.  However, STDP is increasingly being viewed as essentially a product of the artificial conditions used to invoke it \cite{ShouvalWangWittenberg10}, and with realistic spike trains it reduces to a more classical form of Hebbian learning \cite{SongMillerAbbott00,ShouvalWangWittenberg10}.  In any case, there are very few, if any, computational models relying strictly on any form of Hebbian learning that achieve the signature capabilities of the mammalian neocortex (e.g., high-performance visual object recognition).  This is because Hebbian learning relies on strictly local, correlational signals that, while generally useful, are not actually sufficient to converge on the more complex representations that are often needed for real-world problems \cite{OReillyMunakata00,OReillyMunakataFrankEtAl12}.

Instead, most of the computationally-motivated work on high-performance learning algorithms has centered on error-driven learning mechanisms, most prominently the venerable error backpropagation algorithm \cite{RumelhartHintonWilliams86b}, and the support vector machine (SVM) \cite{svmcites}.  Error backpropagation for example is mathematically derived to solve whatever problem is posed to it, through the incremental process of error minimization, and has proven its value in a large number of models.  There has been a recent resurgence of interest in error backpropagation, which plays a central role in recent {\em deep learning} networks \cite{HintonSalakhutdinov06,CiresanMeierGambardellaEtAl10,CiresanMeierSchmidhuber12,BengioCourvilleVincent13}.  Indeed, the best performance on a range of different benchmark tasks has been achieved with purely backpropagation networks, combined with a number of modern optimizations and tricks \cite{CiresanMeierSchmidhuber12,BengioCourvilleVincent13}.

Error-driven learning mechanisms can also achieve, more effectively, the same computational goal as Hebbian learning: developing an internal model of the structure of the external world.  Early work focused on the autoencoder model, where a network learns to reconstruct the inputs it receives, typically through a more compressed internal representation \cite{Pollack90,RumelhartMcClelland86}, and this is how backpropagation learning is often used in deep learning networks \cite{BengioCourvilleVincent13}.  More recently, the Bayesian framework has been leveraged for this same purpose, to create a {\em generative model} of the environment \cite{DayanHintonNealEtAl95,Friston05,Friston10}.  This framework can be traced all the way back to the notion of {\em recognition by synthesis} advanced by Helmholtz, and has been exploited in other neural learning frameworks as well \cite[e.g.,]{RaoBallard99,CarpenterGrossberg87}.  Interestingly, these Bayesian models have not demonstrated the practical learning performance of the backpropagation-based networks, which explains the resurgence of interest in error backpropagation.

In this paper, we focus on a specific form of the reconstructive learning idea, which we call {\em predictive learning}, where instead of just reproducing or generating the current inputs, the network learns to predict what will happen next.  This idea was pioneered initially in the backpropagation framework, using simple recurrent networks (SRNs) \cite{Elman90,Elman91,Jordan89}, and has been leveraged in other frameworks as well \cite{HawkinsBlakeslee04,GeorgeHawkins09}.  Generative and predictive forms of learning are particularly compelling because they provide a ubiquitous source of learning signals: if you attempt to predict everything that happens next, then every single moment is a learning opportunity.  This kind of pervasive learning can for example explain how an infant seems to magically acquire such a sophisticated understanding of the world, despite their seemingly inert overt behavior \cite{ElmanBatesJohnsonEtAl96} --- they are becoming increasingly expert predictors of what they will see next, and as a result, developing increasingly sophisticated internal models of the world.  In the context of language, predictive learning can drive the induction of sophisticated internal representations of syntactic categories, for example \cite{Elman91}.

We can now situate the LeabraTI model in the above context.  The Leabra framework on which LeabraTI is based, is founded on a biologically-plausible form of error backpropagation \cite{OReilly96}, combined with a Hebbian associative learning mechanism (Leabra stands for Local, Error-driven and Associative, Biologically Realistic Algorithm --- it is pronounced like ``Libra'' and is intended to connote the {\em balance} of different factors).  A recent revision \cite{OReillyMunakataFrankEtAl12} includes a unified error-driven and Hebbian learning mechanism derived directly from a highly detailed biophysical model of STDP \cite{UrakuboHondaFroemkeEtAl08}.  The core idea for the error-driven learning aspect is simply that we learn when our expectations are violated.  This implies that we are constantly forming such expectations, so that violations thereof will provide valuable learning signals.  Specifically, learning in Leabra is driven by the difference between two subsequent states of activations in the network: the expectation or {\em minus phase}, compared with the outcome or {\em plus phase}.  This use of a temporal difference to drive learning is borrowed from the original Boltzmann machine and variants \cite{AckleyHintonSejnowski85,HintonMcClelland88a,GallandHinton90}, and contrasts with other frameworks that use the difference between top-down and bottom-up signals to drive learning \cite{Friston05,Friston10,GeorgeHawkins09,RumelhartHintonWilliams86b}.  Computationally, the LeabraTI framework just adds one key additional mechanism to standard Leabra: the ability to sustain the temporal context information necessary for generating predictions based on prior inputs and states of the network.  In this respect, it is very similar to the SRN modification of error backpropagation, and indeed there is a direct mathematical relationship between LeabraTI and the SRN, as we explain below.  However, LeabraTI generalizes the role of temporal context representations in important ways beyond the SRN framework, and provides a detailed biological account for how such context representations can be implemented in the neocortex.

In the remainder of this paper, we introduce the specific ideas for how the thalamocortical and neocortical laminar structure supports temporal integration and learning, and then review the relevant empirical literature across the biology and behavioral domains that bears on the specific computationally and biologically-motivated claims of the LeabraTI model.  Then, we present an application of the model to object recognition in cluttered visual scenes, and conclude with a discussion including comparisons with other related approaches.


\section{Thalamocortical Mechanisms for Temporal Integration Learning}

We begin with a summary overview of how the LeabraTI model works, in terms of differential functional roles for superficial and deep layers of the neocortex, and loops through the thalamus, and the temporal dynamics of information flow through this circuit.  Then, we explore each of these elements in greater depth, in relation to available biological and cognitive data.

\subsection{Overview of LeabraTI Model}

\begin{figure}
  \centering\includegraphics[width=6in]{figs/fig_leabra_ti_func}
  \caption{\footnotesize The temporal evolution of information flow in a LeabraTI model predicting visual sequences, over a period of three alpha cycles of 100 msec each.  The  Deep context maintains the prior 100 msec information while the Superficial generates a prediction (in the minus phase) about what will happen next.  Learning occurs in comparing this prediction with the plus phase outcome, which generates an updated activity pattern in the Super layers.  Thus, prediction error is a temporally extended quantity, not coded explicitly in individual neurons.}
  \label{fig.leabra_ti}
\end{figure}

\begin{figure}
  \centering\includegraphics[width=4in]{figs/fig_leabra_ti_v1_v2_detailed}
  \caption{\small Anatomical connectivity supporting the LeabraTI model. Super (II/II) layers have extensive connectivity within and between areas, and do the primary information processing.  Deep layer V integrates contextual information within and between areas, and 5b bursting neurons only update this context signal every 100 msec, driving a new state of firing on the layer VI tonic firing neurons.  These layer VI neurons sustain the context through recurrent projections through the thalamic relay cells (TRC), which also communicate the context up to the Super neurons (via IV) to support generation of the next prediction.}
  \label{fig.leabra_ti_bio}
\end{figure}

The LeabraTI model of temporal integration in the neocortex leverages the unique properties of the thalamocortical microcircuit (Figures~\ref{fig.leabra_ti}, \ref{fig.leabra_ti_bio}).  This model makes detailed contact with a wide range of biological and functional data, often with counterintuitive predictions. 
Specifically, the two major claims are:

{\bf 1. Time is discretized into roughly 100 msec intervals}, which correspond to the widely observed alpha rhythm in posterior neocortex.  Computationally, this discretization is important for giving the system sufficient time for bidirectional constraint-satisfaction processing to generate reasonable expectations about what will happen next. Because this processing itself takes time, it is not possible to be continuously generating these predictions, and hence the input must be discretely sampled.  Biologically, the properties of the layer 5b deep neocortical neurons, together with dynamics of thalamic neurons, are thought to underlie the generation of the alpha rhythm \abbrevcite{LorinczKekesiJuhaszEtAl09,FranceschettiGuatteoPanzicaEtAl95,BuffaloFriesLandmanEtAl11,LuczakBarthoHarris13}.  Psychologically, there is increasing evidence for a discretization of perception at the alpha scale \abbrevcite{VanRullenKoch03b}. % We review this literature in more detail below.

% ROR: note that start of this para provides brief overview, then rest is 
% important details (I think..)
{\bf 2. Temporal context is sustained in the deep neocortical layers}, while the superficial layers continuously integrate new information, along with this sustained deep context.  Specifically, we argue that the burst-firing dynamics of the layer 5b neurons result in two phases of activity in the deep layers, corresponding to the minus and plus phases of the Leabra algorithm.  The 5b neurons burst fire in the plus phase, driving the updating of downstream layer 6 neurons, which then drive input back down to the thalamus, which then comes back up to the same cortical area, both to layer 6 and up to layer 4 and from there onto the superficial layers (Figure~\ref{fig.leabra_ti_bio}). However, in the minus phase, the 5b neurons are relatively quiescent, and this protects the layer 6 neurons from further influences, enabling them to continue to represent the temporal context from the prior plus phase.  The sustained firing of this layer 6 signal then feeds into the superficial layers, which integrate this prior temporal context with information from all over the rest of the cortex (via inter-areal bidirectional excitatory projections), to produce an expectation about what will happen next.  Then, whatever does happen, happens, and that constitutes the plus phase against which the prior expectation is compared, to drive error-driven learning.  The STDP-based {\em XCAL} learning mechanism in Leabra \cite{OReillyMunakataFrankEtAl12} automatically computes this comparison in a biologically-plausible fashion.  This minus-plus phase oscillation at the alpha frequency thus constitutes the fundamental ``clock cycle'' of cortical computation, according to this framework.


\subsection{Computational Properties, in Relation to the SRN}

As noted above, at a purely computational level, the use of temporal context in LeabraTI to inform new predictions is very similar to the role of the context layer in a simple recurrent network (SRN) \cite{Elman90,Elman91,Jordan89}.  In effect, we hypothesize that the time step for updating an SRN-like context layer is the 100 msec alpha cycle, and during a single alpha cycle, considerable bidirectional constraint satisfaction neural processing is taking place within a LeabraTI network.  This contrasts with the standard SRN, which is typically implemented in a feedforward backpropagation network, where each time step and context update corresponds to a single feedforward activation pass through the network.  Despite this important difference, and several others that we discuss below, there are some critical computational lessons that we adopt directly from the SRN.

One of the most powerful features of the SRN is that it enables error-driven learning, instead of arbitrary parameter settings, to determine how prior information is integrated with new information.  Thus, SRN networks can learn to hold onto some important information for a relatively long interval, while rapidly updating other information that is only relevant for a shorter duration \cite[e.g.,]{CleeremansServanSchreiberMcClelland89,Cleeremans93}.  This same flexibility obtains in our LeabraTI model.  Furthermore, because this temporal context information is hypothesized to be present throughout the entire neocortex (in every microcolumn of tissue), the LeabraTI model provides a more pervasive and interconnected form of temporal integration compared to the SRN, which typically just has a single temporal context layer associated with the internal ``hidden'' layer of processing units.

An extensive computational analysis of what makes the SRN work as well as it does, and explorations of a range of possible alternative frameworks, has led us to an important general principle: {\em future outcomes determine what is relevant from the past}.  At some level, this may seem obvious, but it has significant implications for predictive learning mechanisms based on temporal context.  It means that you cannot learn what information to encode in a temporal context representation at the time when that information is currently active.  Instead, you must learn what information is relevant from that temporal context on the basis of what happens next.  This explains the peculiar power of the otherwise strange property of the SRN: the temporal context information is preserved as a {\em direct copy} of the state of the hidden layer units on the previous time step (Figure~\ref{fig.srn_vs_ti}), and then learned synaptic weights integrate that copied context information into the next hidden state (which is then copied to the context again, and so on).  This enables the error-driven learning taking place in the {\em current} time step to determine how context information from the {\em previous} time step is integrated.  And the simple direct copy operation eschews any attempt to shape this temporal context itself, instead relying on the learning pressure that shapes the hidden layer representations to also shape the context representations.  In other words, this copy operation is essential, because there is no other viable source of learning signals to shape the nature of the context representation itself (because these learning signals require future outcomes, which are by definition only available later).

\begin{figure}
  \centering\includegraphics[width=3in]{figs/fig_srn_ti_compute}
  \caption{\small How TI computation compares to the SRN mathematically. {\bf a)} In a standard SRN, the context (deep layer biologically) is a copy of the hidden activations from the prior time step, and these are held constant while the hidden layer (superficial) units integrate the context through learned synaptic weights.  {\bf b)} In LeabraTI, the deep layer performs the weighted integration of the soon-to-be context information from the superficial layer, and then holds this integrated value, and feeds it back as an additive net-input like signal to the superficial layer.  The context net input is pre-computed, instead of having to compute this same value over and over again.  This is more efficient, and more compatible with the diffuse interconnections among the deep layer neurons.  Layer 6 projections to the thalamus and back recirculate this pre-computed net input value into the superficial layers (via layer 4), and back into itself to support maintenance of the held value.}
  \label{fig.srn_vs_ti}
\end{figure}

The direct copy operation of the SRN is however seemingly problematic from a biological perspective: how could neurons copy activations from another set of neurons at some discrete point in time, and then hold onto those copied values for a duration of 100 msec, which is a reasonably long period of time in neural terms (e.g., a rapidly firing cortical neuron fires at around 100 Hz, meaning that it will fire 10 times within that context frame).  Surprisingly, as summarized above and detailed below, the appropriate mechanisms seem to exist: the phasic bursting of the layer 5b pyramidal neurons acts like a kind of ``gate'' for the updating of the temporal context information, which is maintained through the layer 6 thalamocortical recurrent loops, and communicated up to the superficial layers through layer 4.  However, this biology is more compatible with a particular rotation of the SRN-style computation (Figure~\ref{fig.srn_vs_ti}), where the context is immediately sent through the adaptive synaptic weights that integrate this information, and what is fed back through the layer 6 thalamocortical loops is actually the pre-computed net input from the context onto a given hidden unit, not the raw context information itself.  Computationally, and metabolically, this is a much more efficient mechanism, because the context is, by definition, unchanging over the 100 msec alpha cycle, and thus it makes more sense to pre-compute the synaptic integration, rather than repeatedly re-computing this same synaptic integration over and over again.  Specifically, the extensive collateral connectivity among deep layer 5 neurons is what computes this synaptic integration of the context signal, as we review in the next section.

Mathematically, the LeabraTI context computation is identical to the SRN, despite the rotation of how the computation is performed, as described in detail in Appendix A.  However, this rotation does raise other questions about how the learning of the synaptic weights into the deep layers takes place, which we discuss later in the section on Outstanding Questions.  Also, the LeabraTI model differs from the classic SRN in a number of important ways, many by virtue of it being a Leabra model, with extensive bidirectional connectivity, inhibitory competition within layers, and a biologically-based ion conductance-based activation function \cite{OReillyMunakata00,OReillyMunakataFrankEtAl12}.  We elaborate on these network-level issues next.

\subsection{Network Architecture and Implicit vs. Explicit Prediction}

% todo: include a figure??

One particularly important difference between LeabraTI and a standard SRN arises because of the bidirectional connectivity in LeabraTI and the temporally distributed error-driven learning mechanism, which allows us to use a single input layer to represent both the expectation and outcome, interleaved across time (as shown in Figure~\ref{fig.leabra_ti}).  Furthermore, higher (deeper) layers in the network contribute both to {\em recognition} of the current inputs, and to the {\em prediction} of the next ones, by virtue of their bidirectional interconnectivity with the lower layers.  In contrast, a standard SRN has a fixed input layer feeding through the hidden layer to generate a prediction of the input at the next time step, over an entirely separate set of output units, with no such bidirectional interactions.  Thus, the LeabraTI model provides a more natural mechanism for predictive learning in the brain: the predictions are generated over the very same layers that represent the outcomes against which those predictions are compared.

However, this issue of interleaving predictions vs. outcomes over an input layer raises some important issues, both computationally and biologically, and we have found that a more {\em implicit} form of prediction (which one could perhaps term {\em anticipation} or {\em preparation}) resolves these issues, while retaining much of the same computational power of the full {\em explicit} prediction framework.  The distinction between explicit and implicit prediction centers around whether the input layer is driven by external input during the minus phase.  For explicit prediction, the input layer in the minus phase can only be driven by top-down activation from within the network itself, which thus must be fully responsible for predicting what will happen in the plus phase, when the external input drives the input layer.  For implicit prediction, we allow the input to be driven externally in the minus phase (as well as the plus phase), but with a dynamic parameterization that makes this input have a weaker impact on the rest of the network in the minus phase compared to the plus phase.  In effect, the implicit form is a graded generalization of the explicit case: the input is only partially predicted in the minus phase.  However, from a computational learning perspective, the network still benefits from the error-driven learning signal based on the difference between these two phases: it will still learn to drive the network state in the minus phase to be as similar as possible to that in the plus phase, which is qualitatively similar to the error signal from the explicit prediction case, just weaker in magnitude.

The main computational problem with full explicit prediction case is that for complex real-world input signals (e.g., visual images impinging on primary visual cortex, as we simulate in  the model described later), we do not think that the network can or should be capable of accurately predicting the next input.  Instead, there are many reasons to believe that a primary function of cortical processing is to actively discard massive amounts of information that comes in through the senses, so that only the most relevant and refined signals are retained and processed further \cite{OReillyMunakata00,OReillyMunakataFrankEtAl12}.  In this case, the problem is that there will be a strong and persistent error signal generated by these fuzzy, incomplete predictions, and that will wreak havoc with the error-driven learning mechanism: these error signals will swamp everything else, constantly driving synaptic weights to extremes, and preventing more subtle information from being learned.  In contrast, the implicit prediction case relieves the network from the burden of predicting every last detail of the input, and merely requires that the internal network state learn to be {\em compatible} with the new inputs.

From a biological perspective, this implicit prediction framework provides a much better fit to what we know about the behavior of neurons in V1 (primary visual cortex).  Specifically, there is no evidence that the thalamic inputs to V1 (from the LGN) are turned off during the putative minus phase of the alpha cycle.  However, there is a solid basis for a dynamic modulation of the strength of bottom-up signals from V1 to V2 and other higher layers, which comes from the same layer 5b bursting neurons that we think drive updating of the temporal context representations.  These 5b neurons also drive trans-thalamic projection pathways between areas (e.g., V1 to V2), in parallel to the direct cortico-cortical projections from superficial pyramidals.  Thus, receiving neurons in area V2 will experience a continuous input from V1 superficial neurons throughout the minus and plus phases of the alpha cycle, along with an extra phasic burst in the plus phase.  This constitutes the dynamic parameterization mentioned above, and it ensures that more bottom-up signal is present during the plus phase.  The error-driven learning will then drive the extant top-down and recurrent activation during the minus phase to more closely approximate this bottom-up-heavy plus phase signal.  In practice, we capture this differential parameterization with two different multipliers on synaptic connectivity strengths in the minus and plus phases, and we show later that it is highly effective in driving learning in the network.

Next, we address the biological connections of our framework in greater depth, starting with the basic functional neuroanatomy, followed by electrophysiological and behavioral data.

\subsection{Neurobiology of the Thalamocortical Loops}

We start by reviewing some of the key findings regarding the anatomical and physiological properties of the thalamocortical circuits, as summarized in Figure~\ref{fig.leabra_ti_bio}.  We draw heavily upon integrative reviews \cite[e.g.,]{Thomson10,ThomsonLamy07,SchubertKotterStaiger07,ShermanGuillery06,DouglasMartin04,RocklandPandya79}, which establish the following well-accepted conclusions:
\begin{itemize}
\item Activation generally flows in from the thalamus up to cortical layer 4, and from there up to superficial layers 2/3, and then down to deep layer 5, and finally to layer 6, which predominantly receives from layer 5.  Layer 6 in turn projects back down to the thalamus, which reciprocates with projections back up to layer 6, and up to layer 4.  Thus, despite various possible shortcuts along the way, there is a predominant directionality to activation propagation through the circuit. % Dean: use the phrase ``canonical microcircuit'' here
\item There are two sublamina of layer 5 neurons, 5a and 5b, and each such sublamina contains regular spiking (RS) and intrinsic bursting (IB) subtypes.  The 5a neurons are distinctive in having extensive intermixing within their own subtype (i.e., 5a projecting to 5a, both within and across columns; \nopcite{SchubertKotterStaiger07}).  In layer 5b, the IB subtype has extensive lateral connectivity, and broad receptive fields, whereas the RS subtype has more focal within-column connectivity \cite{SchubertKotterStaiger07}.  The 5b IB intrinsic bursting dynamics occur typically at around the alpha frequency \cite{FranceschettiGuatteoPanzicaEtAl95,FlintConnors96,SilvaAmitaiConnors91,ConnorsGutnickPrince82}.  Our interpretation of this data is that the 5a neurons help to integrate the contextual net input information (per Figure~\ref{fig.srn_vs_ti}), with the 5b intrinsic bursting neurons also doing considerable integration, and then providing the critical timing for when context is updated (at the end of the plus phase).  And 5b neurons instigate a trans-thalamic projection to higher areas \cite{ShermanGuillery06}, consistent with the implicit prediction framework.
\item Consistent with the hypothesized integrative role, the layer 5 neurons generally exhibit broader, integrative tuning compared to superficial layer neurons \cite{SchubertKotterStaiger07}.
\item There are at least three subtypes of pyramidal neurons in layer 6: two types of corticothalamic (CT) and one type of corticocortical (CC) \cite{Thomson10}.  The upper-layer 6 subtype of CT projects to the thalamic area that reciprocally innervates this cortical area (e.g., LGN for V1), and it exhibits a regular spiking profile.  Our hypothesis is that these regular spiking thalamic projecting neurons continuously rebroadcast the integrated context signal that they just received from their strong 5b input projections (until the 5b neurons burst again in the next alpha cycle).  Interestingly, these CT neurons exhibit facilitating short-term dynamics, unlike all other pyramidal neurons, which exhibit depressing dynamics \cite{Thomson10} --- this is suggestive of a specialized function for this cell type, which fits well with this need for short-term maintenance over the alpha cycle.  The other subtype of CT neurons receive more strongly from layer 6 collaterals \cite{ZarrinparCallaway06}, and project to other thalamic targets (e.g., to secondary areas).  These layer 6 collaterals originate largely from the CC subtype, which receives primarily from deep layers (5a, 5b, 6), and projects almost exclusively laterally to other layer 6 neurons.  Interestingly, these layer 6 CC neurons exhibit a strong intrinsic bursting, rapidly accommodating activity pattern, in contrast to the CT regular spiking --- they might represent an additional contributor to the alpha-phase gating dynamic attributed to 5b neurons, perhaps with different overall connectivity.  In addition to our hypothesized primary function for the CT neurons, recent data suggests they may also be involved in feedback gain normalization \cite{OlsenBortoneAdesnikEtAl12}.
\item Thalamic relay neurons that receive from layer 6 project back up to layer 6, and also up to layer 4, in a focal, reciprocal, point-to-point fashion \cite{ShermanGuillery06,Thomson10}.  We hypothesize that this projection sends the integrated contextual signal for a single microcolumn back up to that same microcolumn, with projections into layer 6 serving to sustain the contextual signal over the ensuing alpha cycle, and those into layer 4 playing the key role of providing the context net input to the integrative superficial layer ``hidden'' neurons.  In addition, layer 6 neurons also project directly to layer 4, and while these projections are relatively weak \cite{HirschMartinez06b}, they do activate a metabotropic glutamate receptor (mGluR) that produces sustained depolarization \cite{LeeSherman09} --- this is another possible route for sustained context information to drive layer 4 firing.
\end{itemize}

In summary, it seems that the somewhat peculiar computational demands of the LeabraTI model fit well with the known features of the thalmocortical circuit.  Furthermore, this framework provides a potential answer to the important question as to why there is all this considerable complexity and differentiation of function within the neocortical layers.  It is likely that at least a few other distinct functional mechanisms are embedded within this circuitry, so there is always more work to be done, but at least the present proposal provides some degree of synthesis of a range of otherwise disconnected biological properties, and a number of further testable predictions that are enumerated in the Discussion section.


\subsection{Electrophysiology of Superficial and Deep Layer Neurons}

\begin{figure}
  \centering\includegraphics[width=6in]{figs/fig_luczak_bartho_harris13_fig6f}
  \caption{\small Discretization of a continuous tone stimulus in deep layer 5 neurons at the alpha frequency (Figure 6F from Luczak et al, 2013).  This is consistent with the discretized alpha-frequency updating of temporal context representations in the deep layers in the LeabraTI model.}
  \label{fig.sound_disc_alpha}
\end{figure}

Building upon the core functional anatomy reviewed above, we now examine studies that have recorded activity of superficial and deep layer cortical neurons in behavioral tasks, which provides a broader perspective on the functional dynamics of the thalamocortical loops.  For example, one landmark study showed that superficial cortical layers exhibit mainly gamma frequency power (peak $\sim$50 Hz), whereas deep cortical layers exhibit mainly alpha frequency power (peak $\sim$10 Hz) \cite{BuffaloFriesLandmanEtAl11}.  This is consistent with our framework, where the deep layer context representations are updated at the alpha frequency.  More direct evidence comes from a recording of deep layer 5 neurons in auditory cortex during presentation of sustained tone stimuli \cite{LuczakBarthoHarris13}.  These neurons exhibited alpha-frequency segmentation of this continuous input stimulus, just as we would expect from our model (Figure~\ref{fig.sound_disc_alpha}).  Other studies employing depth electrodes to simultaneously record from multiple layers within a patch of cortex have found large differences in the spectral coherence of superficial and deep neurons \cite{MaierAdamsAuraEtAl10}, as would be predicted by LeabraTI.

A similar experimental paradigm expands on these findings by demonstrating cross-frequency coupling between gamma and alpha spectra localized to superficial and deep layers, respectively \cite{SpaakBonnefondMaierEtAl12}. The cross-frequency coupling was characterized by a clear nesting of gamma activity within alpha cycles, suggesting that deep neurons' alpha coherence might subserve a task-independent $\sim$10 Hz duty cycle for continuous integration by superficial neurons.  This suggests that alpha activity (from deep cortical layers) provides the broader temporal context within which superficial layer processing operates \cite{VanRullenKoch03b,JensenBonnefondVanRullen12}.  Finally, there is evidence that the strong 10 Hz coherence of deep neurons persists even with constant sensory stimulation \textit{in vivo}: \incite{MaierAuraLeopold11} found Layer 5 potentials that were not phase-locked to visual stimulation with a strong 10 Hz component as long as the stimulus was present. 

\subsection{Behavioral Evidence of Discretized Perception at the Alpha Frequency}

Next, we turn to behavioral data indicating at least some degree of discretization of perception at the alpha frequency.  The question of whether perception is discrete or continuous has occupied the literature for over 30 years \cite{VarelaToroJohnEtAl81,VanRullenKoch03b,JensenBonnefondVanRullen12}. Our everyday experience suggests that perception is undeniably continuous, but a number of phenomena support the idea that it is discretized at the alpha frequency, at least to some extent.  Critically, the LeabraTI predicts a mixture of both continuous and discrete aspects to perception, because the superficial layers in the model are continuously updating, whereas it is only the deep layers that are discretized at the alpha frequency.  Overall, we find that this mixture of both continuous and discretized aspects provides a better fit to the data than either extreme case alone.

\incite{VarelaToroJohnEtAl81} provided the first demonstration of discretization at the alpha rhythm by presenting two light stimuli with a short, but constant inter-stimulus interval such that they would be perceived as either illuminating simultaneously or sequentially with equal probability. When the illumination was triggered at the peak of an alpha cycle, subjects generally perceived the lights as simultaneous compared to sequentially when they were presented at the trough.  Although subsequent replication of \posscite{VarelaToroJohnEtAl81} results has failed \cite{VanRullenKoch03b}, there are a number of other phenomena that are consistent with discretized perception at the alpha frequency. 

For example, the {\em wagon wheel illusion}, where a rotating spoked wheel appears to switch direction at certain speeds due typically to a strobing light or as a result of discrete movie frames, also occurs under continuous illumination, and is maximal at 10 Hz \cite{VanRullenReddyKoch05,VanRullenReddyKoch06}.  This suggests that the alpha rhythm might impose a similar temporal aliasing effect, due to a frame-like discretization process.  A recent investigation indicated that a static wagon wheel-like stimulus also flickers at rates estimated at $\sim$10 Hz when viewed in the visual periphery outside the scope of overt attention \cite{SokoliukVanRullen13}.  Relatedly, illusory jitter of high-contrast edges has been shown to occur at 10 Hz \cite{AmanoArnoldTakedaEtAl08}. All of these effects have been correlated with increased alpha-band power over the visual cortices and can be accounted for by a model that posits periodic fluctuations in sensory efficacy at approximately 10 Hz.

\begin{figure}
  \centering
  \includegraphics[width=2.75in]{figs/fig_alpha_phase_onset_mathewson}
  \caption{\small Correlation between at-threshold stimulus detection and alpha frequency cycle phase, showing a clear phasic dependency.  Reproduced from \protect\incite{MathewsonFabianiGrattonEtAl10}}
  \label{fig.alpha_phase_onset}
\end{figure}

The phase of ongoing alpha has activity has also been related to sensory processing efficacy. Errors made processing at-threshold stimuli have been suggested to arise from alpha phase at stimulus onset \cite{BuschDuboisVanRullen09,MathewsonFabianiGrattonEtAl10,VanrullenDubois11}. Specifically, analyses that split data based on whether stimuli were successfully perceived have indicated opposing phases for successful versus unsuccessful detection (Figure~\ref{fig.alpha_phase_onset}). For undetected stimuli, the alpha cycle was approximately in phase at target onset and 180$^{\circ}$ out of phase 50 ms later when processing begins in primary visual cortex \cite{NowakBullier97}. LeabraTI suggests that this impairment is specifically due to the prediction computation during the trough (minus phase) of the alpha cycle, when bottom-up inputs are at their weakest.  In contrast, stimuli arriving in time for the plus-phase peak of the alpha cycle obtain a facilitated boost in processing due to the effects of layer 5b bursting on updating context representations and driving transthalamic circuits.

Given the importance of alpha phase in shaping the envelope of successful perception, it seems that there should be a mechanism in place to synchronize the alpha phase to the timing of environmental stimuli.  Indeed, intrinsic oscillations have been shown to phase-lock to exogenous rhythmic visual and auditory stimulation \cite{FujiokaTrainorLargeEtAl09,SpaakdeLangeJensen14,CalderoneLakatosButlerEtAl14}. This phase-locking ensures that environmental events coincide with key neural events that affect sensory efficacy, like layer 5b bursts that update temporal context. Furthermore, higher frequency bands such as gamma are entrained to alpha phase, which causes momentary modulations in their power \cite{LakatosKarmosMehtaEtAl08,SpaakBonnefondMaierEtAl12}. \incite{MathewsonPrudhommeFabianiEtAl12} presented subjects with a train of stimuli that either were either rhythmic, and thus reliably predicted the temporal onset of a masked probe, or were arhythmic and unpredictable. Rhythmic stimulus trains caused entrained endogenous alpha oscillations and as a result, probes that occurred in either 100 ms or 200 ms after the probe were less susceptible to the effects of masking, due to their processing falling within the peak of the next alpha cycle. fMRI studies have indicated that hemodynamic responses are sensitive to alpha phase \cite{ScheeringaMazaheriBojakEtAl11}. For example, presenting facial motion at alpha-band frequencies results in higher overall responses in face-selective regions \cite{SchultzBrockhausBulthoffEtAl13}. Alpha phase might also play a role in gating long-range functional connectivity between visual processing areas \cite{HanslmayrVolbergWimberEtAl13}. % TODO: summary statement?

Phase resetting is thought to underly the alpha rhythm's environmental phase-locking properties \cite{CalderoneLakatosButlerEtAl14}. Phase resetting also provides flexibility for the rhythm can align to unexpected salient stimuli that capture attention. For example, salient flashes can cause fluctuations in perceptual efficacy that oscillate at 10 Hz after the flash onset \cite{LandauFries12}. Salient sounds can also cross-modally reset alpha in the visual cortices with a similar fluctuations in accuracy after the event \cite{FiebelkornFoxeButlerEtAl11,RomeiGrossThut12}. In an effect reminiscent of the original simultaneity/sequentiality paradigm of \incite{VarelaToroJohnEtAl81}, \incite{ShamsKamitaniShimojo02} showed that salient sounds played during a persistent stimulus can cause the perception of multiple flashes of the stimulus. The framework developed here suggests that this illusory perception is due to the cross-modal reset of the alpha rhythm by sound, which begins with a trough during which visual information from the sensory periphery is suppressed in favor of prediction, creating an illusory gap in the persistent visual stimulus. 

There are a handful of other phenomena that might be related to discretization of perception specifically at alpha frequency. The heavily-studied attentional blink, for example, occurs when observers respond to the onset of a target in a stream of stimuli presented at a constant rate (typically around 10 Hz). Successful detection of the target causes impairment of subsequent targets for several hundred milliseconds. These results can at least partially be accounted for by alpha entrainment and power/phase properties after the onset of the first target, and recent data using EEG recording to quantify these properties support this idea \cite{JansonDeVosThorneEtAl14,ZaunerFellingerGrossEtAl12}. Eye movements also elicit fluctuations in sensory efficacy that bear a highly similar profile to those elicited by the alpha rhythm. Saccades are characterized by extreme suppression of visual activity and visual processing following a saccade is enhanced relative to baseline \cite{MelloniSchwiedrzikRodriguezEtAl09,ParadisoMeshiPisarcikEtAl12}. Monkey electrophysiology has indicated that this is due to a synchronization of 
neural activity 100 ms after a fixation \cite{MaldonadoBabulSingerEtAl08}, likely from a phase reset in one or more frequency bands \cite{RajkaiLakatosChenEtAl08,ItoMaldonadoSingerEtAl11}.

To summarize the data reviewed here, there are a number of visual phenomena that show periodicities in perceptual processing, most of which fall in the 10hz alpha band. Furthermore, mechanisms exist to reset and lock the phase of alpha to important environmental stimuli. In the context of LeabraTI, this phase alignment is necessary for ensuring that important environmental inputs are coded during the peak of the alpha cycle (LeabraTI's plus phase) at which point neuronal excitability is at its highest. Less important inputs can be down-modulated in favor of internally generated predictions that occur during the trough of the alpha cycle (LeabraTI's minus phase) that can be compared with what actually happens during  next to drive learning.

\begin{figure}
  \centering
  \includegraphics[width=3.25in]{figs/fig_oscillation_strength_threshold}
  \caption{\small Effect of level of synaptic input drive on measured power of a given oscillatory frequency: stronger synaptic input results in a lower effective firing threshold compared to the oscillation dynamics (represented by the red sine wave), and thus drives neural firing at a wider range of phases in the cycle, reducing measured power.  Only if the synaptic input is strongly phase locked can this dynamic be avoided.}
  \label{fig.oscillation_str_phase}
\end{figure}

One remaining question concerns the relationship between various cognitive states and alpha power. For example, alpha power tends to decrease with increases in attention \cite{MathewsonLlerasBeckEtAl11,JensenBonnefondVanRullen12}.  This is consistent with the general finding that lower-frequency oscillations are enhanced in inverse proportion to information processing engagement (e.g., these oscillations are greatest during sleep and while the eyes are closed, and become less pronounced with attentive processing \cite{REFS}).  Mechanistically, this occurs because the oscillations are entrained in part by intrinsic membrane properties of neurons, which are more influential when synaptic drive is weaker (Figure~\ref{fig.oscillation_str_phase}).  Put another way, stronger synaptic drive is able to break through the underlying low-frequency modulation for a greater percentage of the cycle, thus reducing the overall measured power of the low-frequency band.

This reduction in alpha with greater processing engagement may seem contradictory to the LeabraTI framework, where you might expect the opposite to hold, given the overall importance of alpha modulation for information processing in this framework.  However, it is essential to carefully consider the full set of circuits, and how they are measured in a given imaging paradigm, to make clear predictions from the LeabraTI framework.  First, as emphasized above, the alpha signal is driven by the deep layer 5b neurons, and the extent to which it dominates the overall power spectrum depends on the relative impact of these neurons compared to the superficial neurons, which are thought to be continuously firing.  If they have weak synaptic drive, then these superficial neurons will likely reflect some of the 5b drive.  But as the drive on the superficial neurons increases, they should exhibit less alpha energy, for the basic neurophysiological reasons just described (Figure~\ref{fig.oscillation_str_phase}).  The same arguments hold for all the other neurons in the system (e.g., 5a and 6 regular spiking neurons).  Thus, the basic underlying alpha modulation, which we think persists across all states of cortical processing and arousal levels, will nevertheless be drowned out and less pervasive during states of high arousal and attention.  Even lamina-specific recordings will be contaminated by a mixture of neuron types --- the true test of the LeabraTI model requires detailed neuron-specific recordings and tracking of how information signals across these neurons update as a function of the overall alpha cycle.


% , we do not think so --- it would be hard to prevent these basic forces from having these effects.  Nevertheless, it might still be possible to measure differences in alpha power as a function of stimulus predictability, if the overall level of synaptic drive is controlled for (e.g., attention and other factors are equated).  We discuss this possibility later in the section on predictions. 
% Dean: Not sure what the point of this paragraph is... there is some relevant stuff from JensenBonnefondVanRullen12 (see also SchroederLakatos09) that suggests that a TI-like process for salient unattended things (e.g., pedestrian in your periphery while driving). Looking back at the EEG effects, most of the ones that show a phase diff are only when alpha is HIGH (probably due to drifting attention)

% doesn't have much to do with discretization, but shows alpha's influence on temporal predictability and reaction time -- worth mentioning here or somewhere...
% on oscillations and prediction -- StefanicsHangyaHernadiEtAl10, RohenkohlNobre11, BesleSchevonMehtaEtAl11, SchroederLakatosKajikawaEtAl08 (review)

\section{The Computational Benefits of Predictive Learning in Early Vision}

% Dean: Worth including Nuo Li and Jim Dicarlo stuff with temporal learning building invariance in IT neurons

We now turn to a computational exploration of LeabraTI learning that illustrates one important way in which predictive learning can unfold in an ecologically and cognitively plausible fashion, while also imparting clear functional benefits in a well-controlled comparison against a system that does not leverage predictive learning.  Specifically, we build upon our existing work on invariant object recognition \cite{OReillyWyatteHerdEtAl13} by investigating the impact of predictive learning on shaping the early visual representations upon which higher-level object recognition builds.  In our prior model, we found that complex cluttered backgrounds substantially impaired recognition performance compared to objects on blank backgrounds, and this is  true of machine vision algorithms more generally, which hover around 60-70\% accuracy on recognition tasks with complex backgrounds --- this is well below human performance on such tasks.  Here, we investigate the hypothesis that figure-ground segregation processes operating in early visual areas (i.e., areas V2 and V3; \nopcite{QiuVonDerHeydt05}) play a critical role in enabling the high performance of human vision.  A scene is first parsed into a structured encoding of the relevant surfaces, organized in relative depth, and then object recognition operates on relevant subsets of such surfaces, effectively filtering out the irrelevant background features.  In particular, we hypothesize that people (and other mammals) learn these early visual representations by simply predicting how a visual scene will unfold over time, which drives development of representations of the separable surface elements, and how they move over time.

To test whether LeabraTI can learn to encode the structure of visual scenes, we presented our model with rendered 3D movies of 100 different objects from our CU3D 100 object data set (\verb\http://cu3d.colorado.edu\; \nopcite{OReillyWyatteHerdEtAl13}), with each object tumbling through space in front of complex visual backgrounds (Figure~\ref{fig.tumbling_objs}).  These movies were rendered online as the model learns, with randomly-generated motion parameters, backgrounds, objects, etc, so there is a high level of variability and broad, combinatorial sampling of the space.  Replicating the known developmental maturation of areas, we start with just the lower visual areas for this initial training (V1, V2, and V3), and then add higher areas to perform object recognition based on these early visual representations.  V1 has two different spatial frequency channels (high: 24x24 locations with 4 oriented edge features x 2 polarities per location, medium: 12x12 locations with same filters), and V2 samples a 4x4 retinotopically organized window into the corresponding V1 (Figure~\ref{fig.objrec_ti_net}B).  V3 is bidirectionally connected to the full extent of V2, to enable it to develop flexible higher-order representations across the whole visual field, and it also has an important topographic projection from a layer that summarizes the overall activity pattern over V2 ({\em V2 sum}), which helps V3 develop topographically organized representations.  We suggest that this V2 sum layer corresponds to the pulvinar of the thalamus, due to it having similar overall properties \cite{pulivnarcites}.

We ran the model in both the explicit and implicit prediction modes (as described earlier), each of which provided useful informative results.  In the explicit prediction mode, the network attempted to generate in the minus phase the entire V1 representation for the next time step.  This allows us to compute the overall accuracy of the network as result of training, to determine how much of this challenging task it can actually perform.  Despite the highly variable inputs, we found that the model learns to an overall average cosine of 0.65 to predict the next input frame.  A perfect prediction would be 1.0, so this is well below that, but nevertheless, the cosine is computed over every oriented edge feature in the V1 encoding of the image, so we do not expect anywhere near perfection with that level of detail.  Visually, it is clear that the minus phase predictions capture the overall shape and motion of the objects quite well (Figure~\ref{fig.objrec_ti_net}A).

However, the synaptic weights learned in the explicit prediction mode were highly saturated due to the strong, persistent error signal, and this error signal interfered with the ability of the network to learn the subsequent object recognition task.  These limitations are remedied by the implicit prediction version.  However, with this version, we cannot say exactly how well the network is able to predict the next input.  Nevertheless, we can track the phase-based differences across the V2 and V3 layers, and observe that they decrease systematically over training (indicating that the network is correctly anticipating the plus phase state in the minus phase), while avoiding degenerate representations where this would be trivially true (e.g., having the same pattern active at all times, or no pattern active at all).  Furthermore, when we then add the object recognition task, we find that learned weights do a much better job in filtering out cluttered backgrounds than those trained with explicit prediction learning.

\begin{figure}
  \centering\includegraphics[width=4in]{figs/fig_objrec_tumble_pliers_4steps}
  \caption{\footnotesize Example of tumbling object (pliers) in front of complex background, used to train LeabraTI model, from binocular eyes of ``emer'' virtual robot.  The model predicts input in the next alpha cycle, learning from prediction errors.}
  \label{fig.tumbling_objs}
\end{figure}

\begin{figure}
  \centering\begin{tabular}{l}
    A\\
    \includegraphics[width=2in]{figs/fig_objrec_ti_tumble_minus_plus}\\
    B\\
    \includegraphics[width=2in]{figs/fig_objrec_ti_fffb_network_nov3}
  \end{tabular}
  \caption{\footnotesize {\bf A)} Example of expectation vs.\/ outcome on higher-res V1 layer for a wrench tumbling through space (with no background) (.56 cosine). {\bf B)} Full TI-based LVis network, processing a person figure against a background.}
  \label{fig.objrec_ti_net}
\end{figure}

\begin{figure}
  \centering\includegraphics[width=2in]{figs/fig_objrec_ti_fffb_ti_effects_epc}
  \caption{\footnotesize Object recognition accuracy for 4 conditions (from bottom): {\bf No TI}: no LeabraTI training of V2 on tumbling objects; {\bf TI}: with LeabraTI V2 training; {\bf TI+FG In}: TI plus explicit figure training, provided as input during object recognition (``cheating''); {\bf No BG}: no backgrounds at all.}
  \label{fig.objrec_ti_results}
\end{figure}

After 25,000 trials of visual predictive learning, we then expanded the network to include the higher-level object recognition layers (V4, IT; Figure~\ref{fig.objrec_ti_net}), and trained the network to label the foreground object, as in our previous LVis networks.  In these initial tests, we used only 10 objects, with a reduced range of tumble, to ensure good overall recognition performance from our initial small-scale model.  We also compared this {\em TI} network with others that had no initial predictive learning experience (labeled {\em No TI}), and another model that had no backgrounds during object recognition training ({\em No BG}), to be contrasted against the {\em BG} case with backgrounds.  The results (Figure~\ref{fig.objrec_ti_results}) show that the initial predictive learning experience produces a substantial improvement in object recognition performance (just over 10\%), compared to the network with no such experience.  Furthermore, the resulting {\em TI} performance was closer overall to that of the networks without backgrounds, than it was to the {\em No TI} model, indicating that the TI learning had gone a reasonable portion of the way toward extracting the figure objects from the backgrounds.  Finally, the trained {\em TI} weights did also benefit performance for the {\em No BG} condition model as well, compared to the {\em No TI} model in the {\em No BG} condition.  However, this benefit appears to be smaller than that on the {\em BG} condition.

Overall, these results are very encouraging, and suggest that implicit predictive learning in the LeabraTI framework may provide an important approach to learning the 3D and dynamic structure of the visual world, over the early layers of the visual system.  This is an important advance compared to the existing models in this domain, which generally rely on hand-coded representations \cite{Li,Niebur}.  We think that the representations that develop through predictive learning will be more powerful and capture a much broader range of structural constraints and regularities in the visual world.  In part, this belief is based on our general failure to find object recognition advantages for our implementations of existing hand-coded figure-ground representation structures (unpublished research), as compared to the significant advantages found here for the TI network.  Despite these encouraging results, considerable work remains to be done -- in particular with respect to the complexity of the scenes that are learned by the TI mechanism, and thus the corresponding amount of visual structure that can be learned.  TODO: talk about action etc.


\section{Discussion}

In summary, we think that every 100ms the neocortex is integrating prior context, captured in deep layers across the whole system, to generate a systematic set of predictions across all areas, ultimately down to all the primary sensory cortices, which is then followed immediately by a wave of Alpha-peak activation that reflects the actual perceptual inputs, propagated "upward" throughout the rest of the cortex, in a way that depends in part on the 5b intrinsic bursting dynamics. This also results in the updating of the deep-layer context state, which persists through to the next predictive phase, and enables the subsequent predictions to integrate prior context in an effective way. The superficial cortical layer neurons experience an alternating state of prediction followed by perception, which drives learning in these neurons to develop more accurate predictions over time.

\subsection{Comparison with other Frameworks}

\subsubsection{Friston's Free Energy Model}

Perhaps the one other model that attempts to make direct contact between a computational learning theory and detailed properties of the neocortex, across a range of different phenomena, is Friston's free energy model \cite{Friston05,Friston10}.  This model is based on minimizing free energy, which amounts to minimizing the discrepancy between the predictions of an internal model of the world, and what the world actually presents to the senses.  It leverages the Bayesian generative model framework, and has been offered as a unifying principle for a very wide range of phenomena \cite{Friston10}.  At a very broad level, the focus on generative learning is compatible with the predictive learning in LeabraTI, so many of the general arguments in favor of this form of learning are shared between these frameworks.

However, the more detailed claims about mechanism differ significantly between the two.  For example, Friston's model posits the existence of a subset of neocortical neurons that explicitly encode the error between the top-down prediction and the bottom-up input, whereas in LeabraTI this error is only implicit in the temporal difference in activations across the minus vs. plus phase.  We are not aware of any compelling evidence in favor of these error-coding neurons, and indeed a comprehensive review focused on Bayesian models of the brain concluded that the available evidence from direct neural recordings was not compatible with such neurons \cite{KerstenMamassianYuille04}.  Instead, this review concluded that the evidence was more compatible with a role for top-down connections in sharpening and focusing lower-level representations \cite{LeeYangRomeroEtAl02}, which is directly compatible with the excitatory constraint-satisfaction dynamic in LeabraTI.  Moreover, this sharpening dynamic can readily explain the fMRI data showing reduced activity for expected vs. unexpected inputs, which has otherwise been offered as evidence of error-coding neurons \cite{FristonErrorCodingfMRI}.  Furthermore, these error-coding neurons require top-down connections to be inhibitory, but the anatomy and physiology does not support this directly (all long-range connections in the neocortex are excitatory), requiring various more complex schemes that appear to have yet to be worked out in detail.  In contrast, the LeabraTI model fits the known anatomy and physiology as reviewed earlier. 

Perhaps the most important difference between these frameworks is that LeabraTI includes a specific mechanism for maintaining and integrating temporal context information (to support predictions based on recent prior history), whereas the free energy model does not address this issue directly.  Relatedly, that model does not address the functional significance of the alpha frequency dynamics, specifically in the deep layers, which are a major focus of LeabraTI, and clearly have some functional significance as documented in an increasing number of studies, as reviewed above.  Lastly, we note that there are no existing models of the free energy framework, or related Bayesian-style ``Helmholtz machines'' \cite{DayanHintonNealEtAl95} that have achieved the kind of powerful learning results demonstrated by backpropagation-based hierarchical models \cite{CiresanMeierSchmidhuber12,BengioCourvilleVincent13}, including our Leabra-based object recognition model \cite{OReillyWyatteHerdEtAl13} and the LeabraTI version described earlier.  This suggests that, for some reason, the error backpropagation framework just works better than the Bayesian framework.

\subsubsection{Deep Networks}

As noted in the introduction, there has been a resurgence in error backpropagation and other forms of error-driven learning in deep neural networks (having many hidden layers), driven in part by the impressive gains in performance that these models have demonstrated \cite{HintonSalakhutdinov06,CiresanMeierGambardellaEtAl10,CiresanMeierSchmidhuber12,BengioCourvilleVincent13}.  Some of the earlier versions of these models used an incremental learning process for training nested autoencoders, where each additional layer learned to encode the information present in the previous layer \cite{HintonSalakhutdinov06,BengioLamblinPopoviciEtAl07}.  This generative autoencoder framework is generally compatible with the predictive learning in LeabraTI, and we also have found that incrementally training layers (which is compatible with the known developmental progression in cortical plasticity; \nopcite{ShragerJohnson96}) results in better overall performance.  However, more recent models, including those developed by Hinton and colleagues (e.g., the unpublished results of the ImageNet 2012 challenge, \verb\http://www.image-net.org/challenges/LSVRC/2012/\) show that the best performance results from a pure feedforward backpropagation network, combined with a number of important extra ``tricks'' \cite{CiresanMeierGambardellaEtAl10,CiresanMeierSchmidhuber12,BengioCourvilleVincent13}.  These extra tricks include biologically-supported properties present in Leabra (e.g., winner-take-all learning and sparse representation pressure), and biologically implausible but computationally powerful techniques, most importantly the sharing of synaptic weights across all the neurons in a given layer (known as a {\em convolutional} network).  In general, none of this work has been concerned with biological plausibility issues.  We are not aware of any version of these deep networks that incorporates any kind of temporal context mechanism such as that present in LeabraTI, or the powerful form of predictive learning that it enables.

Nevertheless, the impressive computational power of these deep neural networks based on error backpropagation provide an important demonstration that the core learning principles built into Leabra can solve challenging problems that we know the neocortex actually solves.  Furthermore, the bidirectional connectivity present in Leabra, and not in these other deep networks, provides an additional computationally powerful mechanism, although it also incurs a significant computational cost, making it more difficult to achieve the large scales used in the purely feedforward models.  We are optimistic that a scaled-up version of the object recognition model presented in this paper will demonstrate the power of predictive learning in LeabraTI, over and above the power of purely error-driven learning in object recognition.  

\subsubsection{Hawkins' Model}

The importance of predictive learning and temporal context are central to the theory advanced by Jeff Hawkins \cite{HawkinsBlakeslee04}.  This theoretical framework has been implemented in various ways, and mapped onto the neocortex \cite{GeorgeHawkins09}.  In one incarnation, the model is similar to the Bayesian belief networks described above, and many of the same issues apply (e.g., this model predicts explicit error coding neurons, among a variety of other response types).  Another more recent incarnation (described apparently only on a white paper available on the website \verb\numenta.org\) diverges from the Bayesian framework, and adopts various heuristic mechanisms for constructing temporal context representations and performing inference and learning.  We think our model provides a computationally more powerful mechanism for learning how to use temporal context information, and learning in general, based on error-driven learning mechanisms.  At the biological level, the two frameworks appear to make a number of distinctive predictions that could be explicitly tested, although enumerating these is beyond the scope of this paper.

\subsubsection{Granger's Model}

Another model which has a detailed mapping onto the thalamocortical circuitry was developed by Granger and colleagues \cite{RodriguezWhitsonGranger04}.  The central idea behind this model is that there are multiple waves of sensory processing, and each is progressively differentiated from the previous ones, producing a temporally-extended sequence of increasingly elaborated categorical encodings ({\em iterative hierarchical clustering}).  The framework also hypothesizes that temporal sequences are encoded via a chaining-based mechanism.  In contrast with the LeabraTI framework, there does not appear to be a predictive learning element to this theory, and nor does it address the functional significance of the alpha frequency modulation of these circuits.  It seems quite likely that this framework makes at least a few distinctive predictions from LeabraTI that could be empirically tested, though again this is beyond the scope of this paper.

\subsection{Predictions}

Enumerate a few biological and behavioral predictions, that have not already been tested..

\subsection{Unresolved Issues}

* learning of deep layer prjns -- must be driven by 2/3 delta signal -- some kind of column-level broadcast of that?  how might that occur?

* incorporating entrainment into the TI model -- definitely there in biology, important for real-time flexibility, but trial-based segmentation is convenient

* higher-level layers integrate over longer time periods?  e.g., theta in hippocampus..  what about beta in pfc??


The question as to how clean the alpha frequency firing of layer 5b neurons must be for the temporal context representation to work effectively still remains --- this will require a more biologically detailed version of our model to address at a computational level.  It is quite possible that different cortical columns can update at different time intervals, providing more of a overlapping, distributed temporal encoding, so a perfectly strong alpha power oscillation is unlikely to be necessary.  Furthermore, given the variety of neuron subtypes within the deep layers, any local field potential or other gross measure of alpha power will not be a pure measure of just the critical layer 5b neurons.  Thus, modulations of alpha power within otherwise well-controlled conditions, and not raw overall levels, should be the most informative for testing predictions of the model. % Dean: This paragraph does not belong here!


\subsection{Conclusions}


\bibliography{ccnlab}

\end{document}
